{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0239fde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/deep/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from optuna_dashboard import run_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b989047a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = Path(\"/Users/maryamhomayoon/PycharmProjects/optuna/optuna-examples/db.sqlite3\")\n",
    "DB_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "STORAGE = f\"sqlite:///{DB_PATH.as_posix()}\"\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE.type == \"mps\":\n",
    "    torch.mps.manual_seed(SEED)\n",
    "else:\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "BATCHSIZE = 128\n",
    "CLASSES = 10\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 10\n",
    "# N_TRAIN_EXAMPLES = BATCHSIZE * 30\n",
    "# N_VALID_EXAMPLES = BATCHSIZE * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40d273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist():\n",
    "    # Load FashionMNIST dataset.\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(DIR, train=True, download=True, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(DIR, train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=BATCHSIZE,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d29b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for simple model with only linear layer\n",
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and #dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 10)\n",
    "    layers = []\n",
    "    in_out_features = [] \n",
    "\n",
    "    in_features = 28 * 28\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(f\"n_units_l{i}\", 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_out_features.append((in_features, out_features))\n",
    "        # p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        # layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    in_out_features.append((in_features, CLASSES))\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers) , in_out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for simple linear model\n",
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    model, out = define_model(trial)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Generate the optimizers.\n",
    "    # optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    # lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    # optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    lr = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    latency = 0.0\n",
    "\n",
    "    for i, (in_f, out_f) in enumerate(out):\n",
    "        latency += estimate_linear_latency(in_f, out_f,num_cores=1, batch=BATCHSIZE)\n",
    "    trial.set_user_attr(\"latency_ms_est\", latency)\n",
    "\n",
    "    # Get the FashionMNIST dataset.\n",
    "    train_loader, valid_loader = get_mnist()\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # # Limiting training data for faster epochs.\n",
    "            # if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "            #     break\n",
    "\n",
    "            data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)    # since we only have linear layers we need to flatten the input first\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # # Limiting validation data.\n",
    "                # if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                #     break\n",
    "                data, target = data.view(data.size(0), -1).to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(valid_loader.dataset)\n",
    "\n",
    "        # trial.report(accuracy, epoch)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value.\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    return accuracy, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a364037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for jsut a formulated latency\n",
    "def estimate_linear_latency(in_features, out_features, num_cores=1):\n",
    "    forward = in_features * out_features + out_features # + bias\n",
    "    backward = 2 * forward # grad w.r.t input, grad w.r.t to weight, grad w.r.t bias so we roughly say cost of backward is twice as much of forward\n",
    "    optimizer = 3 * forward # adam have to do some arithmetic calculations to update each weight so we assume that the cost of those are 3\n",
    "    latency = (forward + backward + optimizer) / num_cores \n",
    "    \n",
    "    return latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for jsut a formulated latency\n",
    "def estimate_conv_latency(input_channels, output_channels, input_height, input_width, kernel_size, stride=1, num_cores=1):\n",
    "    # to formulate the cost of conv layer we check how many output positions we have and how many arithmatic calculations we need to do\n",
    "\n",
    "    # Output spatial size\n",
    "    output_height = (input_height + stride - 1) // stride\n",
    "    output_width  = (input_width  + stride - 1) // stride\n",
    "\n",
    "    # Cost of computing ONE output value\n",
    "    kernel_area = kernel_size * kernel_size\n",
    "    operations_per_output_value = input_channels * kernel_area + 1 # + bias\n",
    "\n",
    "    # Number of output values\n",
    "    number_of_output_values = output_channels * output_height * output_width\n",
    "\n",
    "\n",
    "    forward = operations_per_output_value * number_of_output_values\n",
    "    backward = 2 * forward\n",
    "    optimizer = 3 * forward\n",
    "    latency = (forward + backward + optimizer) / num_cores\n",
    "\n",
    "    return latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc754bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for jsut a formulated latency\n",
    "def estimate_maxpool_latency(channels, input_height, input_width, kernel_size=2, stride=2, num_cores=1):\n",
    "    # Estimate arithmetic cost of MaxPool2d for ONE input sample\n",
    "\n",
    "    # Output spatial size\n",
    "    output_height = input_height // stride\n",
    "    output_width = input_width // stride\n",
    "\n",
    "    kernel_area = kernel_size * kernel_size\n",
    "\n",
    "    # Comparisons per output value\n",
    "    operations_per_output_value = kernel_area - 1\n",
    "\n",
    "    number_of_output_values = channels * output_height * output_width\n",
    "\n",
    "    forward_cost = operations_per_output_value * number_of_output_values\n",
    "    backward_cost = forward_cost    # Backward (no optimizer)\n",
    "    latency = (forward_cost + backward_cost) / num_cores\n",
    "\n",
    "    return latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for jsut a formulated latency\n",
    "def estimate_global_avg_pool_latency(channels, input_height, input_width, num_cores=1):\n",
    "    # Estimate arithmetic cost of Global Average Pooling for ONE input sample.\n",
    "\n",
    "    spatial_area = input_height * input_width\n",
    "    \n",
    "    forward_cost = channels * spatial_area  # Forward: sum + divide\n",
    "    backward_cost = forward_cost    # Backward: distribute gradient\n",
    "    latency = (forward_cost + backward_cost) / num_cores\n",
    "\n",
    "    return latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6aad483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for cycles\n",
    "def estimate_linear_cost(in_features, out_features, num_cores=1, cycles_per_mac=1, cycles_per_add=1):\n",
    "    macs = in_features * out_features\n",
    "    bias_adds = out_features\n",
    "\n",
    "    flops = 2 * macs + bias_adds\n",
    "    cycles = (macs * cycles_per_mac + bias_adds * cycles_per_add) / num_cores\n",
    "\n",
    "    return macs, flops, cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd538a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for cycles\n",
    "def estimate_conv_cost(c_in, c_out, h_in, w_in, kernel_size, stride=1, num_cores=1, cycles_per_mac=1, cycles_per_add=1):\n",
    "    out_h = (h_in + stride - 1) // stride\n",
    "    out_w = (w_in + stride - 1) // stride\n",
    "\n",
    "    kernel_area = kernel_size * kernel_size\n",
    "\n",
    "    macs = c_out * out_h * out_w * (c_in * kernel_area)\n",
    "    bias_adds = c_out * out_h * out_w\n",
    "\n",
    "    flops = 2 * macs + bias_adds\n",
    "    cycles = (macs * cycles_per_mac + bias_adds * cycles_per_add) / num_cores\n",
    "\n",
    "    return macs, flops, cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86c09564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for cycles\n",
    "def estimate_maxpool_cost(channels, h_in, w_in, kernel_size=2, stride=2, num_cores=1, cycles_per_comp=1):\n",
    "    # Output spatial size (no padding pool)\n",
    "    out_h = (h_in - kernel_size) // stride + 1\n",
    "    out_w = (w_in  - kernel_size) // stride + 1\n",
    "\n",
    "    out_values = channels * out_h * out_w\n",
    "    comps_per_out = kernel_size * kernel_size - 1\n",
    "\n",
    "    macs = 0  # no multiply-accumulate\n",
    "    flops = out_values * comps_per_out  # not \"true FLOPs\", but op-count\n",
    "    cycles = (flops * cycles_per_comp) / num_cores\n",
    "\n",
    "    return macs, flops, cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565c22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for cycles\n",
    "def estimate_global_avg_pool_cost(channels, h_in, w_in, num_cores=1, cycles_per_add=1, cycles_per_div=2):\n",
    "    area = h_in * w_in\n",
    "\n",
    "    adds = channels * (area - 1)\n",
    "    divs = channels  # 1 divide per channel\n",
    "\n",
    "    macs = 0\n",
    "    flops = adds + divs  # again: \"ops\" count\n",
    "    cycles = (adds * cycles_per_add + divs * cycles_per_div) / num_cores\n",
    "\n",
    "    return macs, flops, cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with conv and linear while taking care of padding and while trying different strides\n",
    "def define_model(trial):\n",
    "    layers = []\n",
    "    # FashionMNIST is 28x28 grayscale images\n",
    "    in_channels = 1\n",
    "    in_height = 28\n",
    "    in_width = 28\n",
    "\n",
    "    current_features = None\n",
    "    spatial_mode = True     # a flag to control what architecture are allowed and that spatial sturctures of the connected layers makes sense\n",
    "    used_global_pool = False    # we can classify with conv -> global pooling -> classifier(log softmax)\n",
    "\n",
    "    layer_descriptions = []\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 10)\n",
    "\n",
    "    for layer_idx in range(n_layers):\n",
    "        # First layer should be conv or if it's not conv then we need to flatten the input\n",
    "        # last layer should either be linear or we have to apply a conv with 10 output channels and then we have to apply global pooling and then classifier\n",
    "        # 10 channel conv beacause it's FashionMNIST dataset\n",
    "        # if the last global pool is not with 10 out channel then we linearly have it to 10 classes output and then go for classifier\n",
    "\n",
    "        # If spatial is already 1x1, don't allow more spatial ops and go to linear head.\n",
    "        # Either go directly to final head, or start linear layers.\n",
    "        if spatial_mode and (in_height < 2 or in_width < 2):\n",
    "            layers.append(nn.Flatten())\n",
    "            current_features = in_channels * in_height * in_width  # == in_channels\n",
    "            spatial_mode = False\n",
    "\n",
    "            # Option: either add one extra linear layer, or go directly to classes\n",
    "            go_direct = trial.suggest_categorical(f\"go_direct_{layer_idx}\", [True, False])\n",
    "            if not go_direct:\n",
    "                out_features = trial.suggest_int(f\"early_linear_out_{layer_idx}\", 16, 128)\n",
    "                layers.append(nn.Linear(current_features, out_features))\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "                layer_descriptions.append({\n",
    "                    \"type\": \"linear\",\n",
    "                    \"in_features\": current_features,\n",
    "                    \"out_features\": out_features,\n",
    "                })\n",
    "                current_features = out_features\n",
    "\n",
    "            # break out; final head will be added after the loop\n",
    "            break\n",
    "\n",
    "        if layer_idx == 0:\n",
    "            layer_type = \"conv\"\n",
    "        elif spatial_mode:\n",
    "            layer_type = trial.suggest_categorical(\n",
    "                f\"layer_type_{layer_idx}\",\n",
    "                [\"conv\", \"pool\", \"global_pool\", \"linear\"]\n",
    "            )\n",
    "        else:\n",
    "            # Once spatial structure is gone, only Linear is allowed\n",
    "            layer_type = \"linear\"\n",
    "\n",
    "        if layer_type == \"conv\":\n",
    "            out_channels = trial.suggest_categorical(\n",
    "                f\"conv_out_channels_{layer_idx}\", [8, 16, 32, 64, 128]\n",
    "            )\n",
    "            kernel_size = trial.suggest_int(\n",
    "                f\"conv_kernel_{layer_idx}\", 1, 7, step=2  # odd only\n",
    "            )\n",
    "            stride = trial.suggest_categorical(\n",
    "                f\"conv_stride_{layer_idx}\", [1, 2]\n",
    "            )\n",
    "\n",
    "            # Save input shape BEFORE the layer (for latency)\n",
    "            c_in, h_in, w_in = in_channels, in_height, in_width\n",
    "\n",
    "            padding = kernel_size // 2  # works with odd kernels for stride 1 and 2\n",
    "\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            # Output size tracking (matches your \"ceil\" style)\n",
    "            out_h = (h_in + stride - 1) // stride\n",
    "            out_w = (w_in + stride - 1) // stride\n",
    "\n",
    "            in_channels = out_channels\n",
    "            in_height = out_h\n",
    "            in_width = out_w\n",
    "\n",
    "            layer_descriptions.append({\n",
    "                \"type\": \"conv2d\",\n",
    "                \"out_channels\": out_channels,\n",
    "                \"kernel_size\": kernel_size,\n",
    "                \"stride\": stride,\n",
    "                \"input_shape\": (c_in, h_in, w_in),\n",
    "            })\n",
    "\n",
    "        elif layer_type == \"pool\":\n",
    "            c_in, h_in, w_in = in_channels, in_height, in_width\n",
    "\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "            in_height = h_in // 2\n",
    "            in_width = w_in // 2\n",
    "\n",
    "            layer_descriptions.append({\n",
    "                \"type\": \"maxpool2d\",\n",
    "                \"kernel_size\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"input_shape\": (c_in, h_in, w_in),\n",
    "            })\n",
    "\n",
    "        elif layer_type == \"global_pool\":\n",
    "            c_in, h_in, w_in = in_channels, in_height, in_width\n",
    "\n",
    "            layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            layers.append(nn.Flatten())\n",
    "\n",
    "            current_features = c_in\n",
    "\n",
    "            spatial_mode = False\n",
    "            used_global_pool = True\n",
    "\n",
    "            # After flatten, no more spatial dims\n",
    "            in_channels = None\n",
    "            in_height = None\n",
    "            in_width = None\n",
    "\n",
    "            layer_descriptions.append({\n",
    "                \"type\": \"global_avg_pool\",\n",
    "                \"features\": current_features,\n",
    "                \"input_shape\": (c_in, h_in, w_in),\n",
    "            })\n",
    "\n",
    "        else:  # linear\n",
    "            if spatial_mode:\n",
    "                layers.append(nn.Flatten())\n",
    "                current_features = in_channels * in_height * in_width\n",
    "                spatial_mode = False\n",
    "\n",
    "                in_channels = None\n",
    "                in_height = None\n",
    "                in_width = None\n",
    "\n",
    "            out_features = trial.suggest_int(\n",
    "                f\"linear_out_{layer_idx}\", 16, 128\n",
    "            )\n",
    "\n",
    "            layers.append(nn.Linear(current_features, out_features))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            layer_descriptions.append({\n",
    "                \"type\": \"linear\",\n",
    "                \"in_features\": current_features,\n",
    "                \"out_features\": out_features,\n",
    "            })\n",
    "\n",
    "            current_features = out_features\n",
    "\n",
    "    # Final classifier block\n",
    "    if used_global_pool:\n",
    "        layers.append(nn.Linear(current_features, CLASSES))\n",
    "        layer_descriptions.append({\n",
    "            \"type\": \"linear\",\n",
    "            \"in_features\": current_features,\n",
    "            \"out_features\": CLASSES,\n",
    "        })\n",
    "    else:\n",
    "        if spatial_mode:\n",
    "            layers.append(nn.Flatten())\n",
    "            current_features = in_channels * in_height * in_width\n",
    "\n",
    "        layers.append(nn.Linear(current_features, CLASSES))\n",
    "        layer_descriptions.append({\n",
    "            \"type\": \"linear\",\n",
    "            \"in_features\": current_features,\n",
    "            \"out_features\": CLASSES,\n",
    "        })\n",
    "\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers), layer_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redesign of the optuna search space for correct pooling\n",
    "def define_model(trial):\n",
    "    layers = []\n",
    "    in_channels, in_height, in_width = 1, 28, 28    # FashionMNIST is 28x28 grayscale images\n",
    "\n",
    "    current_features = None\n",
    "    spatial_mode = True     # a flag to control what architecture are allowed and that spatial sturctures of the connected layers makes sense\n",
    "    used_global_pool = False\n",
    "\n",
    "    layer_descriptions = []\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 10)\n",
    "\n",
    "    for layer_idx in range(n_layers):\n",
    "        # pick layer type\n",
    "        if layer_idx == 0:\n",
    "            layer_type = \"conv\"\n",
    "        elif spatial_mode:\n",
    "            choices = [\"conv\", \"global_pool\", \"linear\"]\n",
    "            if in_height >= 2 and in_width >= 2:\n",
    "                choices.append(\"pool\")  # add pool only when it's valid\n",
    "            layer_type = trial.suggest_categorical(f\"layer_type_{layer_idx}\", choices)\n",
    "        else:\n",
    "            layer_type = \"linear\"\n",
    "\n",
    "        # build layer\n",
    "        if layer_type == \"conv\":\n",
    "            out_channels = trial.suggest_categorical(\n",
    "                f\"conv_out_channels_{layer_idx}\", [8, 16, 32, 64, 128]\n",
    "            )\n",
    "\n",
    "            # If spatial is tiny, large kernels are mostly padding work.\n",
    "            # This is optional; remove if you want truly weird kernels at 1x1.\n",
    "            if min(in_height, in_width) == 1:\n",
    "                kernel_size = 1\n",
    "            else:\n",
    "                kernel_size = trial.suggest_int(f\"conv_kernel_{layer_idx}\", 1, 7, step=2)\n",
    "\n",
    "            stride = trial.suggest_categorical(f\"conv_stride_{layer_idx}\", [1, 2])\n",
    "\n",
    "            c_in, h_in, w_in = in_channels, in_height, in_width\n",
    "            padding = kernel_size // 2\n",
    "\n",
    "            layers.append(nn.Conv2d(c_in, out_channels, kernel_size, stride=stride, padding=padding))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            # track output size (same-ish padding, odd kernels)\n",
    "            in_height = (h_in + stride - 1) // stride\n",
    "            in_width  = (w_in + stride - 1) // stride\n",
    "            in_channels = out_channels\n",
    "\n",
    "            layer_descriptions.append({\n",
    "                \"type\": \"conv2d\",\n",
    "                \"out_channels\": out_channels,\n",
    "                \"kernel_size\": kernel_size,\n",
    "                \"stride\": stride,\n",
    "                \"input_shape\": (c_in, h_in, w_in),\n",
    "            })\n",
    "\n",
    "        elif layer_type == \"pool\":\n",
    "            # pool is only offered when H,W >= 2\n",
    "            c_in, h_in, w_in = in_channels, in_height, in_width\n",
    "\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "            in_height = h_in // 2\n",
    "            in_width  = w_in // 2\n",
    "\n",
    "            layer_descriptions.append({\n",
    "                \"type\": \"maxpool2d\",\n",
    "                \"kernel_size\": 2,\n",
    "                \"stride\": 2,\n",
    "                \"input_shape\": (c_in, h_in, w_in),\n",
    "            })\n",
    "\n",
    "        elif layer_type == \"global_pool\":\n",
    "            c_in, h_in, w_in = in_channels, in_height, in_width\n",
    "\n",
    "            layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "            layers.append(nn.Flatten())\n",
    "\n",
    "            current_features = c_in\n",
    "            spatial_mode = False\n",
    "            used_global_pool = True\n",
    "\n",
    "            # prevent accidental spatial usage later\n",
    "            in_channels, in_height, in_width = None, None, None\n",
    "\n",
    "            layer_descriptions.append({\n",
    "                \"type\": \"global_avg_pool\",\n",
    "                \"features\": current_features,\n",
    "                \"input_shape\": (c_in, h_in, w_in),\n",
    "            })\n",
    "\n",
    "        else:  # linear\n",
    "            if spatial_mode:\n",
    "                layers.append(nn.Flatten())\n",
    "                current_features = in_channels * in_height * in_width\n",
    "                spatial_mode = False\n",
    "                in_channels, in_height, in_width = None, None, None\n",
    "\n",
    "            out_features = trial.suggest_int(f\"linear_out_{layer_idx}\", 16, 128)\n",
    "            layers.append(nn.Linear(current_features, out_features))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            layer_descriptions.append({\n",
    "                \"type\": \"linear\",\n",
    "                \"in_features\": current_features,\n",
    "                \"out_features\": out_features,\n",
    "            })\n",
    "\n",
    "            current_features = out_features\n",
    "\n",
    "    # final classifier head (always ends in 10 classes)\n",
    "    if used_global_pool:\n",
    "        layers.append(nn.Linear(current_features, CLASSES))\n",
    "        layer_descriptions.append({\n",
    "            \"type\": \"linear\",\n",
    "            \"in_features\": current_features,\n",
    "            \"out_features\": CLASSES,\n",
    "        })\n",
    "    else:\n",
    "        if spatial_mode:\n",
    "            layers.append(nn.Flatten())\n",
    "            current_features = in_channels * in_height * in_width\n",
    "\n",
    "        layers.append(nn.Linear(current_features, CLASSES))\n",
    "        layer_descriptions.append({\n",
    "            \"type\": \"linear\",\n",
    "            \"in_features\": current_features,\n",
    "            \"out_features\": CLASSES,\n",
    "        })\n",
    "\n",
    "    layers.append(nn.LogSoftmax(dim=1))\n",
    "    return nn.Sequential(*layers), layer_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95487b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for conv model\n",
    "def objective(trial):\n",
    "    # Generate the model.\n",
    "    model, out = define_model(trial)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Generate the optimizers.\n",
    "    # optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    # lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    # optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    lr = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    macs_total, flops_total, cycles_total = 0.0, 0.0, 0.0\n",
    "    macs_per_layer, flops_per_layer, cycles_per_layer = [], [], []\n",
    "\n",
    "\n",
    "    for layer in out:\n",
    "\n",
    "        if layer[\"type\"] == \"conv2d\":\n",
    "            c_in, h_in, w_in = layer[\"input_shape\"]\n",
    "            macs, flops, cycles = estimate_conv_cost(c_in=c_in, c_out=layer[\"out_channels\"], h_in=h_in, w_in=w_in, kernel_size=layer[\"kernel_size\"], stride=layer[\"stride\"])\n",
    "\n",
    "        elif layer[\"type\"] == \"maxpool2d\":\n",
    "            c, h, w = layer[\"input_shape\"]\n",
    "            macs, flops, cycles = estimate_maxpool_cost(channels=c, h_in=h, w_in=w, kernel_size=layer[\"kernel_size\"], stride=layer[\"stride\"])\n",
    "\n",
    "        elif layer[\"type\"] == \"global_avg_pool\":\n",
    "            c, h, w = layer[\"input_shape\"]\n",
    "            macs, flops, cycles = estimate_global_avg_pool_cost(channels=c, h_in=h, w_in=w)\n",
    "\n",
    "        elif layer[\"type\"] == \"linear\":\n",
    "            macs, flops, cycles = estimate_linear_cost(layer[\"in_features\"], layer[\"out_features\"])\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        macs_total += macs\n",
    "        flops_total += flops\n",
    "        cycles_total += cycles\n",
    "\n",
    "        macs_per_layer.append(macs)\n",
    "        flops_per_layer.append(flops)\n",
    "        cycles_per_layer.append(cycles)\n",
    "\n",
    "    # store for Optuna dashboard\n",
    "    trial.set_user_attr(\"macs_total\", macs_total)\n",
    "    trial.set_user_attr(\"flops_total\", flops_total)\n",
    "    trial.set_user_attr(\"cycles_total\", cycles_total)\n",
    "    trial.set_user_attr(\"macs_per_layer\", macs_per_layer)\n",
    "    trial.set_user_attr(\"flops_per_layer\", flops_per_layer)\n",
    "    trial.set_user_attr(\"cycles_per_layer\", cycles_per_layer)\n",
    "\n",
    "    # 2nd objective\n",
    "    latency = cycles_total/100000000\n",
    "\n",
    "    # Get the FashionMNIST dataset.\n",
    "    train_loader, valid_loader = get_mnist()\n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # # Limiting training data for faster epochs.\n",
    "            # if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "            #     break\n",
    "\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation of the model.\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "                # # Limiting validation data.\n",
    "                # if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "                #     break\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                output = model(data)\n",
    "                # Get the index of the max log-probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = correct / len(valid_loader.dataset)\n",
    "\n",
    "        # trial.report(accuracy, epoch)\n",
    "\n",
    "        # # Handle pruning based on the intermediate value.\n",
    "        # if trial.should_prune():\n",
    "        #     raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    return accuracy, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78634e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-02-09 13:06:03,406] A new study created in RDB with name: 1.loger time out and latency=cycles/frequency of 100MHz\n",
      "[I 2026-02-09 13:06:40,896] Trial 0 finished with values: [0.3592, 0.00199828] and parameters: {'n_layers': 9, 'conv_out_channels_0': 64, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'global_pool', 'linear_out_2': 114, 'linear_out_3': 108, 'linear_out_4': 39, 'linear_out_5': 78, 'linear_out_6': 117, 'linear_out_7': 69, 'linear_out_8': 58}.\n",
      "[I 2026-02-09 13:07:10,378] Trial 1 finished with values: [0.8957, 0.00287763] and parameters: {'n_layers': 6, 'conv_out_channels_0': 16, 'conv_kernel_0': 3, 'conv_stride_0': 1, 'layer_type_1': 'pool', 'layer_type_2': 'linear', 'linear_out_2': 44, 'linear_out_3': 101, 'linear_out_4': 72, 'linear_out_5': 36}.\n",
      "[I 2026-02-09 13:07:29,573] Trial 2 finished with values: [0.8675, 0.0009409] and parameters: {'n_layers': 1, 'conv_out_channels_0': 8, 'conv_kernel_0': 7, 'conv_stride_0': 2}.\n",
      "[I 2026-02-09 13:07:50,111] Trial 3 finished with values: [0.2992, 0.00037818] and parameters: {'n_layers': 2, 'conv_out_channels_0': 16, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'global_pool'}.\n",
      "[I 2026-02-09 13:08:24,057] Trial 4 finished with values: [0.8339, 0.00080135] and parameters: {'n_layers': 9, 'conv_out_channels_0': 8, 'conv_kernel_0': 5, 'conv_stride_0': 2, 'layer_type_1': 'pool', 'layer_type_2': 'pool', 'layer_type_3': 'linear', 'linear_out_3': 126, 'linear_out_4': 33, 'linear_out_5': 89, 'linear_out_6': 53, 'linear_out_7': 108, 'linear_out_8': 92}.\n",
      "[I 2026-02-09 13:09:29,664] Trial 5 finished with values: [0.7119, 0.29884457] and parameters: {'n_layers': 6, 'conv_out_channels_0': 64, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'conv', 'conv_out_channels_1': 64, 'conv_kernel_1': 3, 'conv_stride_1': 1, 'layer_type_2': 'conv', 'conv_out_channels_2': 64, 'conv_kernel_2': 1, 'conv_stride_2': 2, 'layer_type_3': 'pool', 'layer_type_4': 'global_pool', 'linear_out_5': 61}.\n",
      "[I 2026-02-09 13:11:19,017] Trial 6 finished with values: [0.9066, 0.12470457] and parameters: {'n_layers': 5, 'conv_out_channels_0': 128, 'conv_kernel_0': 5, 'conv_stride_0': 1, 'layer_type_1': 'linear', 'linear_out_1': 98, 'linear_out_2': 60, 'linear_out_3': 103, 'linear_out_4': 127}.\n",
      "[I 2026-02-09 13:11:54,751] Trial 7 finished with values: [0.9049, 0.0166095] and parameters: {'n_layers': 7, 'conv_out_channels_0': 128, 'conv_kernel_0': 3, 'conv_stride_0': 2, 'layer_type_1': 'linear', 'linear_out_1': 55, 'linear_out_2': 121, 'linear_out_3': 20, 'linear_out_4': 94, 'linear_out_5': 80, 'linear_out_6': 125}.\n",
      "[I 2026-02-09 13:12:43,379] Trial 8 finished with values: [0.3619, 0.00328471] and parameters: {'n_layers': 6, 'conv_out_channels_0': 128, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'pool', 'layer_type_2': 'global_pool', 'linear_out_3': 57, 'linear_out_4': 127, 'linear_out_5': 91}.\n",
      "[I 2026-02-09 13:13:04,806] Trial 9 finished with values: [0.293, 4.802e-05] and parameters: {'n_layers': 2, 'conv_out_channels_0': 8, 'conv_kernel_0': 1, 'conv_stride_0': 2, 'layer_type_1': 'global_pool'}.\n",
      "[I 2026-02-09 13:13:33,135] Trial 10 finished with values: [0.886, 0.00295209] and parameters: {'n_layers': 4, 'conv_out_channels_0': 16, 'conv_kernel_0': 5, 'conv_stride_0': 2, 'layer_type_1': 'conv', 'conv_out_channels_1': 16, 'conv_kernel_1': 3, 'conv_stride_1': 2, 'layer_type_2': 'conv', 'conv_out_channels_2': 16, 'conv_kernel_2': 1, 'conv_stride_2': 1, 'layer_type_3': 'linear', 'linear_out_3': 109}.\n",
      "[I 2026-02-09 13:14:10,874] Trial 11 finished with values: [0.9001, 0.0107806] and parameters: {'n_layers': 9, 'conv_out_channels_0': 64, 'conv_kernel_0': 5, 'conv_stride_0': 2, 'layer_type_1': 'linear', 'linear_out_1': 57, 'linear_out_2': 111, 'linear_out_3': 16, 'linear_out_4': 80, 'linear_out_5': 110, 'linear_out_6': 66, 'linear_out_7': 93, 'linear_out_8': 46}.\n",
      "[I 2026-02-09 13:15:01,525] Trial 12 finished with values: [0.9124, 0.01938833] and parameters: {'n_layers': 8, 'conv_out_channels_0': 128, 'conv_kernel_0': 3, 'conv_stride_0': 1, 'layer_type_1': 'pool', 'layer_type_2': 'linear', 'linear_out_2': 33, 'linear_out_3': 102, 'linear_out_4': 103, 'linear_out_5': 68, 'linear_out_6': 111, 'linear_out_7': 27}.\n",
      "[I 2026-02-09 13:15:34,443] Trial 13 finished with values: [0.8705, 0.00289153] and parameters: {'n_layers': 6, 'conv_out_channels_0': 32, 'conv_kernel_0': 3, 'conv_stride_0': 2, 'layer_type_1': 'conv', 'conv_out_channels_1': 8, 'conv_kernel_1': 1, 'conv_stride_1': 2, 'layer_type_2': 'conv', 'conv_out_channels_2': 8, 'conv_kernel_2': 7, 'conv_stride_2': 1, 'layer_type_3': 'linear', 'linear_out_3': 99, 'linear_out_4': 95, 'linear_out_5': 104}.\n",
      "[I 2026-02-09 13:15:54,348] Trial 14 finished with values: [0.8876, 0.00225802] and parameters: {'n_layers': 1, 'conv_out_channels_0': 8, 'conv_kernel_0': 5, 'conv_stride_0': 1}.\n",
      "[I 2026-02-09 13:16:23,231] Trial 15 finished with values: [0.899, 0.01596975] and parameters: {'n_layers': 5, 'conv_out_channels_0': 128, 'conv_kernel_0': 5, 'conv_stride_0': 2, 'layer_type_1': 'linear', 'linear_out_1': 37, 'linear_out_2': 67, 'linear_out_3': 91, 'linear_out_4': 75}.\n",
      "[I 2026-02-09 13:17:09,478] Trial 16 finished with values: [0.7297, 0.10328793] and parameters: {'n_layers': 7, 'conv_out_channels_0': 32, 'conv_kernel_0': 3, 'conv_stride_0': 1, 'layer_type_1': 'conv', 'conv_out_channels_1': 64, 'conv_kernel_1': 5, 'conv_stride_1': 2, 'layer_type_2': 'global_pool', 'linear_out_3': 67, 'linear_out_4': 75, 'linear_out_5': 47, 'linear_out_6': 78}.\n",
      "[I 2026-02-09 13:21:52,777] Trial 17 finished with values: [0.8957, 0.81816814] and parameters: {'n_layers': 4, 'conv_out_channels_0': 64, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'conv', 'conv_out_channels_1': 128, 'conv_kernel_1': 7, 'conv_stride_1': 2, 'layer_type_2': 'linear', 'linear_out_2': 120, 'linear_out_3': 36}.\n",
      "[I 2026-02-09 13:22:13,060] Trial 18 finished with values: [0.8987, 0.0025089] and parameters: {'n_layers': 1, 'conv_out_channels_0': 64, 'conv_kernel_0': 3, 'conv_stride_0': 2}.\n",
      "[I 2026-02-09 13:22:37,964] Trial 19 finished with values: [0.9056, 0.01584207] and parameters: {'n_layers': 3, 'conv_out_channels_0': 8, 'conv_kernel_0': 7, 'conv_stride_0': 2, 'layer_type_1': 'conv', 'conv_out_channels_1': 64, 'conv_kernel_1': 3, 'conv_stride_1': 1, 'layer_type_2': 'linear', 'linear_out_2': 47}.\n",
      "[I 2026-02-09 13:23:25,035] Trial 20 finished with values: [0.9008, 0.1505281] and parameters: {'n_layers': 2, 'conv_out_channels_0': 32, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'conv', 'conv_out_channels_1': 64, 'conv_kernel_1': 3, 'conv_stride_1': 1}.\n",
      "[I 2026-02-09 13:24:00,849] Trial 21 finished with values: [0.8998, 0.01189878] and parameters: {'n_layers': 7, 'conv_out_channels_0': 16, 'conv_kernel_0': 5, 'conv_stride_0': 1, 'layer_type_1': 'linear', 'linear_out_1': 65, 'linear_out_2': 123, 'linear_out_3': 78, 'linear_out_4': 80, 'linear_out_5': 121, 'linear_out_6': 109}.\n",
      "[I 2026-02-09 13:24:44,688] Trial 22 finished with values: [0.8903, 0.01100746] and parameters: {'n_layers': 9, 'conv_out_channels_0': 32, 'conv_kernel_0': 3, 'conv_stride_0': 1, 'layer_type_1': 'linear', 'linear_out_1': 32, 'linear_out_2': 123, 'linear_out_3': 37, 'linear_out_4': 79, 'linear_out_5': 120, 'linear_out_6': 93, 'linear_out_7': 73, 'linear_out_8': 91}.\n",
      "[I 2026-02-09 13:25:23,304] Trial 23 finished with values: [0.6941, 0.00213165] and parameters: {'n_layers': 10, 'conv_out_channels_0': 32, 'conv_kernel_0': 5, 'conv_stride_0': 2, 'layer_type_1': 'global_pool', 'linear_out_2': 105, 'linear_out_3': 119, 'linear_out_4': 62, 'linear_out_5': 127, 'linear_out_6': 29, 'linear_out_7': 26, 'linear_out_8': 117, 'linear_out_9': 36}.\n",
      "[I 2026-02-09 13:25:58,069] Trial 24 finished with values: [0.7683, 0.00675752] and parameters: {'n_layers': 8, 'conv_out_channels_0': 8, 'conv_kernel_0': 3, 'conv_stride_0': 2, 'layer_type_1': 'conv', 'conv_out_channels_1': 64, 'conv_kernel_1': 5, 'conv_stride_1': 2, 'layer_type_2': 'global_pool', 'linear_out_3': 73, 'linear_out_4': 105, 'linear_out_5': 18, 'linear_out_6': 103, 'linear_out_7': 89}.\n",
      "[I 2026-02-09 13:26:32,852] Trial 25 finished with values: [0.8042, 0.0042861] and parameters: {'n_layers': 8, 'conv_out_channels_0': 128, 'conv_kernel_0': 3, 'conv_stride_0': 2, 'layer_type_1': 'pool', 'layer_type_2': 'conv', 'conv_out_channels_2': 8, 'conv_kernel_2': 3, 'conv_stride_2': 2, 'layer_type_3': 'global_pool', 'linear_out_4': 124, 'linear_out_5': 43, 'linear_out_6': 20, 'linear_out_7': 123}.\n",
      "[I 2026-02-09 13:26:59,536] Trial 26 finished with values: [0.6084, 0.0055265] and parameters: {'n_layers': 2, 'conv_out_channels_0': 64, 'conv_kernel_0': 3, 'conv_stride_0': 1, 'layer_type_1': 'global_pool'}.\n",
      "[I 2026-02-09 13:27:39,544] Trial 27 finished with values: [0.8911, 0.0276119] and parameters: {'n_layers': 7, 'conv_out_channels_0': 16, 'conv_kernel_0': 5, 'conv_stride_0': 2, 'layer_type_1': 'conv', 'conv_out_channels_1': 64, 'conv_kernel_1': 7, 'conv_stride_1': 2, 'layer_type_2': 'conv', 'conv_out_channels_2': 32, 'conv_kernel_2': 1, 'conv_stride_2': 1, 'layer_type_3': 'linear', 'linear_out_3': 71, 'linear_out_4': 18, 'linear_out_5': 36, 'linear_out_6': 55}.\n",
      "[I 2026-02-09 13:28:30,860] Trial 28 finished with values: [0.3515, 0.0032908] and parameters: {'n_layers': 10, 'conv_out_channels_0': 128, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'pool', 'layer_type_2': 'global_pool', 'linear_out_3': 30, 'linear_out_4': 119, 'linear_out_5': 94, 'linear_out_6': 48, 'linear_out_7': 39, 'linear_out_8': 41, 'linear_out_9': 18}.\n",
      "[I 2026-02-09 13:29:00,384] Trial 29 finished with values: [0.2684, 0.00029473] and parameters: {'n_layers': 6, 'conv_out_channels_0': 8, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'global_pool', 'linear_out_2': 22, 'linear_out_3': 79, 'linear_out_4': 85, 'linear_out_5': 19}.\n",
      "[I 2026-02-09 13:29:33,080] Trial 30 finished with values: [0.8938, 0.01510656] and parameters: {'n_layers': 6, 'conv_out_channels_0': 64, 'conv_kernel_0': 3, 'conv_stride_0': 2, 'layer_type_1': 'linear', 'linear_out_1': 109, 'linear_out_2': 32, 'linear_out_3': 93, 'linear_out_4': 67, 'linear_out_5': 63}.\n",
      "[I 2026-02-09 13:30:32,298] Trial 31 finished with values: [0.3696, 0.03494264] and parameters: {'n_layers': 8, 'conv_out_channels_0': 32, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'conv', 'conv_out_channels_1': 128, 'conv_kernel_1': 1, 'conv_stride_1': 1, 'layer_type_2': 'pool', 'layer_type_3': 'global_pool', 'linear_out_4': 119, 'linear_out_5': 38, 'linear_out_6': 89, 'linear_out_7': 86}.\n",
      "[I 2026-02-09 13:30:55,902] Trial 32 finished with values: [0.8974, 0.01335946] and parameters: {'n_layers': 2, 'conv_out_channels_0': 8, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'conv', 'conv_out_channels_1': 32, 'conv_kernel_1': 5, 'conv_stride_1': 2}.\n",
      "[I 2026-02-09 13:31:21,729] Trial 33 finished with values: [0.9094, 0.04007818] and parameters: {'n_layers': 2, 'conv_out_channels_0': 64, 'conv_kernel_0': 5, 'conv_stride_0': 2, 'layer_type_1': 'conv', 'conv_out_channels_1': 128, 'conv_kernel_1': 3, 'conv_stride_1': 2}.\n",
      "[I 2026-02-09 13:31:58,168] Trial 34 finished with values: [0.8945, 0.05356035] and parameters: {'n_layers': 7, 'conv_out_channels_0': 16, 'conv_kernel_0': 7, 'conv_stride_0': 2, 'layer_type_1': 'conv', 'conv_out_channels_1': 32, 'conv_kernel_1': 7, 'conv_stride_1': 1, 'layer_type_2': 'linear', 'linear_out_2': 42, 'linear_out_3': 34, 'linear_out_4': 117, 'linear_out_5': 27, 'linear_out_6': 92}.\n",
      "[I 2026-02-09 13:32:29,382] Trial 35 finished with values: [0.8127, 0.00519417] and parameters: {'n_layers': 6, 'conv_out_channels_0': 128, 'conv_kernel_0': 1, 'conv_stride_0': 2, 'layer_type_1': 'conv', 'conv_out_channels_1': 8, 'conv_kernel_1': 3, 'conv_stride_1': 2, 'layer_type_2': 'pool', 'layer_type_3': 'linear', 'linear_out_3': 74, 'linear_out_4': 51, 'linear_out_5': 126}.\n",
      "[I 2026-02-09 13:33:02,036] Trial 36 finished with values: [0.719, 0.02723175] and parameters: {'n_layers': 5, 'conv_out_channels_0': 64, 'conv_kernel_0': 7, 'conv_stride_0': 1, 'layer_type_1': 'conv', 'conv_out_channels_1': 16, 'conv_kernel_1': 1, 'conv_stride_1': 2, 'layer_type_2': 'global_pool', 'linear_out_3': 124, 'linear_out_4': 39}.\n",
      "[I 2026-02-09 13:33:46,865] Trial 37 finished with values: [0.5908, 0.00118457] and parameters: {'n_layers': 10, 'conv_out_channels_0': 32, 'conv_kernel_0': 3, 'conv_stride_0': 2, 'layer_type_1': 'global_pool', 'linear_out_2': 99, 'linear_out_3': 106, 'linear_out_4': 66, 'linear_out_5': 124, 'linear_out_6': 47, 'linear_out_7': 66, 'linear_out_8': 77, 'linear_out_9': 68}.\n",
      "[I 2026-02-09 13:34:19,199] Trial 38 finished with values: [0.7284, 0.02071754] and parameters: {'n_layers': 5, 'conv_out_channels_0': 32, 'conv_kernel_0': 3, 'conv_stride_0': 1, 'layer_type_1': 'conv', 'conv_out_channels_1': 8, 'conv_kernel_1': 3, 'conv_stride_1': 1, 'layer_type_2': 'pool', 'layer_type_3': 'global_pool', 'linear_out_4': 104}.\n",
      "[I 2026-02-09 13:35:09,992] Trial 39 finished with values: [0.3648, 0.00342132] and parameters: {'n_layers': 9, 'conv_out_channels_0': 128, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'global_pool', 'linear_out_2': 99, 'linear_out_3': 47, 'linear_out_4': 87, 'linear_out_5': 69, 'linear_out_6': 42, 'linear_out_7': 53, 'linear_out_8': 125}.\n",
      "[I 2026-02-09 13:35:36,737] Trial 40 finished with values: [0.3484, 0.00161353] and parameters: {'n_layers': 4, 'conv_out_channels_0': 64, 'conv_kernel_0': 1, 'conv_stride_0': 1, 'layer_type_1': 'global_pool', 'linear_out_2': 80, 'linear_out_3': 61}.\n",
      "[I 2026-02-09 13:35:59,259] Trial 41 finished with values: [0.6356, 0.00072904] and parameters: {'n_layers': 4, 'conv_out_channels_0': 32, 'conv_kernel_0': 3, 'conv_stride_0': 2, 'layer_type_1': 'pool', 'layer_type_2': 'global_pool', 'linear_out_3': 90}.\n",
      "[I 2026-02-09 13:36:24,066] Trial 42 finished with values: [0.3443, 0.0008601] and parameters: {'n_layers': 4, 'conv_out_channels_0': 128, 'conv_kernel_0': 1, 'conv_stride_0': 2, 'layer_type_1': 'global_pool', 'linear_out_2': 46, 'linear_out_3': 82}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Trial #2 | accuracy=0.8675 | latency_in_seconds=0\n",
      "MACs per layer   : [76832, 15680]\n",
      "FLOPs per layer  : [155232, 31370]\n",
      "Cycles per layer : [78400.0, 15690.0]\n",
      "Totals:\n",
      "  MACs  : 92512.0\n",
      "  FLOPs : 186602.0\n",
      "  cycles: 94090.0\n",
      "============================================================\n",
      "Trial #3 | accuracy=0.2992 | latency_in_seconds=0\n",
      "MACs per layer   : [12544, 0, 160]\n",
      "FLOPs per layer  : [37632, 12544, 330]\n",
      "Cycles per layer : [25088.0, 12560.0, 170.0]\n",
      "Totals:\n",
      "  MACs  : 12704.0\n",
      "  FLOPs : 50506.0\n",
      "  cycles: 37818.0\n",
      "============================================================\n",
      "Trial #4 | accuracy=0.8339 | latency_in_seconds=0\n",
      "MACs per layer   : [39200, 0, 0, 9072, 4158, 2937, 4717, 5724, 9936, 920]\n",
      "FLOPs per layer  : [79968, 1176, 216, 18270, 8349, 5963, 9487, 11556, 19964, 1850]\n",
      "Cycles per layer : [40768.0, 1176.0, 216.0, 9198.0, 4191.0, 3026.0, 4770.0, 5832.0, 10028.0, 930.0]\n",
      "Totals:\n",
      "  MACs  : 76664.0\n",
      "  FLOPs : 156799.0\n",
      "  cycles: 80135.0\n",
      "============================================================\n",
      "Trial #9 | accuracy=0.2930 | latency_in_seconds=0\n",
      "MACs per layer   : [1568, 0, 80]\n",
      "FLOPs per layer  : [4704, 1568, 170]\n",
      "Cycles per layer : [3136.0, 1576.0, 90.0]\n",
      "Totals:\n",
      "  MACs  : 1648.0\n",
      "  FLOPs : 6442.0\n",
      "  cycles: 4802.0\n",
      "============================================================\n",
      "Trial #11 | accuracy=0.9001 | latency_in_seconds=0\n",
      "MACs per layer   : [313600, 715008, 6327, 1776, 1280, 8800, 7260, 6138, 4278, 460]\n",
      "FLOPs per layer  : [639744, 1430073, 12765, 3568, 2640, 17710, 14586, 12369, 8602, 930]\n",
      "Cycles per layer : [326144.0, 715065.0, 6438.0, 1792.0, 1360.0, 8910.0, 7326.0, 6231.0, 4324.0, 470.0]\n",
      "Totals:\n",
      "  MACs  : 1064927.0\n",
      "  FLOPs : 2142987.0\n",
      "  cycles: 1078060.0\n",
      "============================================================\n",
      "Trial #12 | accuracy=0.9124 | latency_in_seconds=0\n",
      "MACs per layer   : [903168, 0, 827904, 3366, 10506, 7004, 7548, 2997, 270]\n",
      "FLOPs per layer  : [1906688, 75264, 1655841, 6834, 21115, 14076, 15207, 6021, 550]\n",
      "Cycles per layer : [1003520.0, 75264.0, 827937.0, 3468.0, 10609.0, 7072.0, 7659.0, 3024.0, 280.0]\n",
      "Totals:\n",
      "  MACs  : 1762763.0\n",
      "  FLOPs : 3701596.0\n",
      "  cycles: 1938833.0\n",
      "============================================================\n",
      "Trial #14 | accuracy=0.8876 | latency_in_seconds=0\n",
      "MACs per layer   : [156800, 62720]\n",
      "FLOPs per layer  : [319872, 125450]\n",
      "Cycles per layer : [163072.0, 62730.0]\n",
      "Totals:\n",
      "  MACs  : 219520.0\n",
      "  FLOPs : 445322.0\n",
      "  cycles: 225802.0\n",
      "============================================================\n",
      "Trial #18 | accuracy=0.8987 | latency_in_seconds=0\n",
      "MACs per layer   : [112896, 125440]\n",
      "FLOPs per layer  : [238336, 250890]\n",
      "Cycles per layer : [125440.0, 125450.0]\n",
      "Totals:\n",
      "  MACs  : 238336.0\n",
      "  FLOPs : 489226.0\n",
      "  cycles: 250890.0\n",
      "============================================================\n",
      "Trial #19 | accuracy=0.9056 | latency_in_seconds=0\n",
      "MACs per layer   : [76832, 903168, 589568, 470]\n",
      "FLOPs per layer  : [155232, 1818880, 1179183, 950]\n",
      "Cycles per layer : [78400.0, 915712.0, 589615.0, 480.0]\n",
      "Totals:\n",
      "  MACs  : 1570038.0\n",
      "  FLOPs : 3154245.0\n",
      "  cycles: 1584207.0\n",
      "============================================================\n",
      "Trial #41 | accuracy=0.6356 | latency_in_seconds=0\n",
      "MACs per layer   : [56448, 0, 0, 2880, 900]\n",
      "FLOPs per layer  : [119168, 4704, 1568, 5850, 1810]\n",
      "Cycles per layer : [62720.0, 4704.0, 1600.0, 2970.0, 910.0]\n",
      "Totals:\n",
      "  MACs  : 60228.0\n",
      "  FLOPs : 133100.0\n",
      "  cycles: 72904.0\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    directions=[\"maximize\",\"minimize\"],\n",
    "    study_name=\"1.loger time out and latency=cycles/frequency of 100MHz\",\n",
    "    storage=STORAGE,\n",
    "    load_if_exists=True\n",
    ")\n",
    "study.optimize(objective, n_trials=100, timeout=1800)\n",
    "\n",
    "for t in study.best_trials:\n",
    "    acc, latency = t.values  # because you returned (accuracy, latency_cycles)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Trial #{t.number} | accuracy={acc:.4f} | latency_in_seconds={latency:.0000f}\")\n",
    "\n",
    "    flops = t.user_attrs.get(\"flops_per_layer\")\n",
    "    latencies = t.user_attrs.get(\"cycles_per_layer\")\n",
    "    macs = t.user_attrs.get(\"macs_per_layer\")\n",
    "\n",
    "    print(f\"MACs per layer   : {macs}\")\n",
    "    print(f\"FLOPs per layer  : {flops}\")\n",
    "    print(f\"Cycles per layer : {latencies}\")\n",
    "\n",
    "    print(\"Totals:\")\n",
    "    print(\"  MACs  :\", t.user_attrs.get(\"macs_total\"))\n",
    "    print(\"  FLOPs :\", t.user_attrs.get(\"flops_total\"))\n",
    "    print(\"  cycles:\", t.user_attrs.get(\"cycles_total\"))\n",
    "\n",
    "# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "# print(\"Study statistics: \")\n",
    "\n",
    "# print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "# print(\"  Number of finished trials: \", len(study.trials))\n",
    "# print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "# best_trials = study.best_trials\n",
    "# print(f\"\\nNumber of Pareto-optimal trials: {len(best_trials)}\")\n",
    "\n",
    "# for t in best_trials:\n",
    "#     print(f\"  Values: accuracy={t.values[0]:.4f}, latency={t.values[1]:.4f}\")\n",
    "#     print(\"  Params:\")\n",
    "#     for k, v in t.params.items():\n",
    "#         print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a42eabae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bottle v0.13.4 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n",
      "127.0.0.1 - - [09/Feb/2026 13:36:51] \"GET /api/studies/4?after=16 HTTP/1.1\" 200 21692\n",
      "127.0.0.1 - - [09/Feb/2026 13:36:59] \"GET /api/studies HTTP/1.1\" 200 960\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:01] \"GET /api/studies HTTP/1.1\" 200 960\n",
      "/opt/miniconda3/envs/deep/lib/python3.12/site-packages/optuna_dashboard/_importance.py:71: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "/opt/miniconda3/envs/deep/lib/python3.12/site-packages/optuna/importance/_ped_anova/evaluator.py:148: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:03] \"GET /api/studies/6/param_importances?evaluator=ped_anova HTTP/1.1\" 200 838\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:03] \"GET /api/studies/6?after=0 HTTP/1.1\" 200 193401\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:03] \"GET /api/meta HTTP/1.1\" 200 64\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:03] \"GET /api/studies/6/param_importances?evaluator=ped_anova HTTP/1.1\" 200 838\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:13] \"GET /api/studies/6?after=43 HTTP/1.1\" 200 34350\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:23] \"GET /api/studies/6?after=43 HTTP/1.1\" 200 34350\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:33] \"GET /api/studies/6?after=43 HTTP/1.1\" 200 34350\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:43] \"GET /api/studies/6?after=43 HTTP/1.1\" 200 34350\n",
      "127.0.0.1 - - [09/Feb/2026 13:37:54] \"GET /api/studies/6?after=43 HTTP/1.1\" 200 34350\n",
      "127.0.0.1 - - [09/Feb/2026 13:38:04] \"GET /api/studies/6?after=43 HTTP/1.1\" 200 34350\n",
      "127.0.0.1 - - [09/Feb/2026 13:38:14] \"GET /api/studies/6?after=43 HTTP/1.1\" 200 34350\n"
     ]
    }
   ],
   "source": [
    "# Start the Optuna Dashboard server on localhost:8080\n",
    "run_server(STORAGE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
